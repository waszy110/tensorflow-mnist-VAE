{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import mnist_data\n",
    "import os\n",
    "# import beta_vae as vae\n",
    "import vae\n",
    "import plot_utils\n",
    "import glob\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gpu\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "#不平衡 类别\n",
    "choose_c=3\n",
    "#图像尺寸\n",
    "IMAGE_SIZE_MNIST = 28\n",
    "#隐变量维度\n",
    "dim_z=3\n",
    "#alpha classification loss balance\n",
    "alpha=400\n",
    "#隐层节点数\n",
    "n_hidden=500\n",
    "#学习率\n",
    "learn_rate=1e-3\n",
    "#训练轮数\n",
    "n_epochs=100\n",
    "#批量数目\n",
    "batch_size=256\n",
    "#标签样式 one-hot编码\n",
    "one_hot=np.eye(10)\n",
    "np.random.seed(100)\n",
    "dim_img=28*28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0918 10:20:56.365740 140399666411328 deprecation_wrapper.py:119] From /home/yuemei.zhu/tensorflow-mnist-VAE/mnist_data.py:30: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "_, _, _, _, test_data, test_labels = mnist_data.prepare_MNIST_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_hat = tf.placeholder(tf.float32, shape=[None, dim_img], name='input_img')\n",
    "x_label = tf.placeholder(tf.float32, shape=[None, 10], name='img_label')\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rebanlance_dis_label=np.load('./train_rebanlance_dis_label.npy')\n",
    "np.random.shuffle(train_rebanlance_dis_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分类器结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0918 10:20:58.097620 140399666411328 deprecation.py:323] From <ipython-input-6-d38760c62036>:10: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "W0918 10:20:58.103189 140399666411328 deprecation.py:506] From /home/yuemei.zhu/.local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0918 10:20:58.336005 140399666411328 deprecation.py:323] From <ipython-input-6-d38760c62036>:13: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.MaxPooling2D instead.\n",
      "W0918 10:20:58.496815 140399666411328 deprecation.py:323] From <ipython-input-6-d38760c62036>:26: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "W0918 10:20:58.792322 140399666411328 deprecation.py:506] From <ipython-input-6-d38760c62036>:27: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0918 10:20:58.820470 140399666411328 deprecation.py:323] From <ipython-input-6-d38760c62036>:35: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"CNN 的模型函数\"\"\"\n",
    "# 输入层\n",
    "input_layer = tf.reshape(x_hat, [-1, 28, 28, 1])\n",
    "# 第一个卷积层\n",
    "conv1 = tf.layers.conv2d(\n",
    "  inputs=input_layer,\n",
    "  filters=32,\n",
    "  kernel_size=[5, 5],\n",
    "  padding=\"same\",\n",
    "  activation=tf.nn.relu)\n",
    "\n",
    "# 第一个池化层\n",
    "pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "\n",
    "# 第二个卷积层和池化层\n",
    "conv2 = tf.layers.conv2d(\n",
    "  inputs=pool1,\n",
    "  filters=64,\n",
    "  kernel_size=[5, 5],\n",
    "  padding=\"same\",\n",
    "  activation=tf.nn.relu)\n",
    "pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "\n",
    "# 全连接层\n",
    "pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n",
    "dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "dropout=tf.nn.dropout(dense, keep_prob)\n",
    "# dropout = tf.layers.dropout(\n",
    "#   inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "# Logits 层\n",
    "logits = tf.layers.dense(inputs=dropout, units=10)\n",
    "# predicts=tf.argmax(logits,axis=1)\n",
    "# 计算损失（可用于`训练`和`评价`中）\n",
    "cls_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=x_label, logits=logits))\n",
    "cls_optm = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "cls_train_op = cls_optm.minimize(cls_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在类别重新均衡的数据集上训练分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 :  100 epochs loss 2.27 \n",
      "epoch 1 :  100 epochs loss 2.16 \n",
      "epoch 2 :  100 epochs loss 1.84 \n",
      "epoch 3 :  100 epochs loss 1.14 \n",
      "epoch 4 :  100 epochs loss 0.66 \n",
      "epoch 5 :  100 epochs loss 0.48 \n",
      "epoch 6 :  100 epochs loss 0.40 \n",
      "epoch 7 :  100 epochs loss 0.35 \n",
      "epoch 8 :  100 epochs loss 0.32 \n",
      "epoch 9 :  100 epochs loss 0.29 \n",
      "epoch 10 :  100 epochs loss 0.27 \n",
      "epoch 11 :  100 epochs loss 0.26 \n",
      "epoch 12 :  100 epochs loss 0.24 \n",
      "epoch 13 :  100 epochs loss 0.23 \n",
      "epoch 14 :  100 epochs loss 0.22 \n",
      "epoch 15 :  100 epochs loss 0.21 \n",
      "epoch 16 :  100 epochs loss 0.21 \n",
      "epoch 17 :  100 epochs loss 0.20 \n",
      "epoch 18 :  100 epochs loss 0.19 \n",
      "epoch 19 :  100 epochs loss 0.18 \n",
      "epoch 20 :  100 epochs loss 0.18 \n",
      "epoch 21 :  100 epochs loss 0.17 \n",
      "epoch 22 :  100 epochs loss 0.17 \n",
      "epoch 23 :  100 epochs loss 0.16 \n",
      "epoch 24 :  100 epochs loss 0.16 \n",
      "epoch 25 :  100 epochs loss 0.16 \n",
      "epoch 26 :  100 epochs loss 0.15 \n",
      "epoch 27 :  100 epochs loss 0.15 \n",
      "epoch 28 :  100 epochs loss 0.14 \n",
      "epoch 29 :  100 epochs loss 0.14 \n",
      "epoch 30 :  100 epochs loss 0.14 \n",
      "epoch 31 :  100 epochs loss 0.13 \n",
      "epoch 32 :  100 epochs loss 0.13 \n",
      "epoch 33 :  100 epochs loss 0.13 \n",
      "epoch 34 :  100 epochs loss 0.13 \n",
      "epoch 35 :  100 epochs loss 0.12 \n",
      "epoch 36 :  100 epochs loss 0.12 \n",
      "epoch 37 :  100 epochs loss 0.12 \n",
      "epoch 38 :  100 epochs loss 0.12 \n",
      "epoch 39 :  100 epochs loss 0.12 \n",
      "epoch 40 :  100 epochs loss 0.11 \n",
      "epoch 41 :  100 epochs loss 0.11 \n",
      "epoch 42 :  100 epochs loss 0.11 \n",
      "epoch 43 :  100 epochs loss 0.11 \n",
      "epoch 44 :  100 epochs loss 0.11 \n",
      "epoch 45 :  100 epochs loss 0.10 \n",
      "epoch 46 :  100 epochs loss 0.10 \n",
      "epoch 47 :  100 epochs loss 0.10 \n",
      "epoch 48 :  100 epochs loss 0.10 \n",
      "epoch 49 :  100 epochs loss 0.10 \n",
      "epoch 50 :  100 epochs loss 0.10 \n",
      "epoch 51 :  100 epochs loss 0.10 \n",
      "epoch 52 :  100 epochs loss 0.10 \n",
      "epoch 53 :  100 epochs loss 0.09 \n",
      "epoch 54 :  100 epochs loss 0.09 \n",
      "epoch 55 :  100 epochs loss 0.09 \n",
      "epoch 56 :  100 epochs loss 0.09 \n",
      "epoch 57 :  100 epochs loss 0.09 \n",
      "epoch 58 :  100 epochs loss 0.09 \n",
      "epoch 59 :  100 epochs loss 0.09 \n",
      "epoch 60 :  100 epochs loss 0.09 \n",
      "epoch 61 :  100 epochs loss 0.09 \n",
      "epoch 62 :  100 epochs loss 0.08 \n",
      "epoch 63 :  100 epochs loss 0.08 \n",
      "epoch 64 :  100 epochs loss 0.08 \n",
      "epoch 65 :  100 epochs loss 0.08 \n",
      "epoch 66 :  100 epochs loss 0.08 \n",
      "epoch 67 :  100 epochs loss 0.08 \n",
      "epoch 68 :  100 epochs loss 0.08 \n",
      "epoch 69 :  100 epochs loss 0.08 \n",
      "epoch 70 :  100 epochs loss 0.08 \n",
      "epoch 71 :  100 epochs loss 0.08 \n",
      "epoch 72 :  100 epochs loss 0.08 \n",
      "epoch 73 :  100 epochs loss 0.08 \n",
      "epoch 74 :  100 epochs loss 0.08 \n",
      "epoch 75 :  100 epochs loss 0.07 \n",
      "epoch 76 :  100 epochs loss 0.07 \n",
      "epoch 77 :  100 epochs loss 0.07 \n",
      "epoch 78 :  100 epochs loss 0.07 \n",
      "epoch 79 :  100 epochs loss 0.07 \n",
      "epoch 80 :  100 epochs loss 0.07 \n",
      "epoch 81 :  100 epochs loss 0.07 \n",
      "epoch 82 :  100 epochs loss 0.07 \n",
      "epoch 83 :  100 epochs loss 0.07 \n",
      "epoch 84 :  100 epochs loss 0.07 \n",
      "epoch 85 :  100 epochs loss 0.07 \n",
      "epoch 86 :  100 epochs loss 0.07 \n",
      "epoch 87 :  100 epochs loss 0.07 \n",
      "epoch 88 :  100 epochs loss 0.07 \n",
      "epoch 89 :  100 epochs loss 0.07 \n",
      "epoch 90 :  100 epochs loss 0.06 \n",
      "epoch 91 :  100 epochs loss 0.06 \n",
      "epoch 92 :  100 epochs loss 0.06 \n",
      "epoch 93 :  100 epochs loss 0.06 \n",
      "epoch 94 :  100 epochs loss 0.06 \n",
      "epoch 95 :  100 epochs loss 0.06 \n",
      "epoch 96 :  100 epochs loss 0.06 \n",
      "epoch 97 :  100 epochs loss 0.06 \n",
      "epoch 98 :  100 epochs loss 0.06 \n",
      "epoch 99 :  100 epochs loss 0.06 \n"
     ]
    }
   ],
   "source": [
    "cls_epochs=100\n",
    "cls_batch_size=128\n",
    "batchs=int(train_rebanlance_dis_label.shape[0]/cls_batch_size)\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer(), feed_dict={keep_prob : 0.9})\n",
    "for epoch in range(cls_epochs):\n",
    "    loss_np=0\n",
    "    for i in range(batchs):\n",
    "        batch_xs_input=train_rebanlance_dis_label[i*cls_batch_size:(i+1)*cls_batch_size,:-10]\n",
    "        batch_xs_label=train_rebanlance_dis_label[i*cls_batch_size:(i+1)*cls_batch_size,-10:]\n",
    "        _, cls_loss_np= sess.run(\n",
    "            (cls_train_op, cls_loss),\n",
    "            feed_dict={x_hat: batch_xs_input,x_label:batch_xs_label,keep_prob : 0.9})\n",
    "        loss_np+=cls_loss_np\n",
    "    print('epoch %d : % d epochs loss %3.2f '% (epoch,cls_epochs,loss_np/batchs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc 0.97 \n",
      "recall :  [[0.99285714 0.9876652  0.97965116 0.88613861 0.98065173 0.98318386\n",
      "  0.9822547  0.97178988 0.97843943 0.95936571]]\n"
     ]
    }
   ],
   "source": [
    "cls_batch_size=2\n",
    "batchs=int(test_data.shape[0]/cls_batch_size)\n",
    "den=np.sum(test_labels,axis=0)\n",
    "num=np.zeros([1,10])\n",
    "for epoch in range(1):\n",
    "#     loss_np=0\n",
    "    corrects=0\n",
    "    for i in range(batchs):\n",
    "        batch_xs_input=test_data[i*cls_batch_size:(i+1)*cls_batch_size]\n",
    "        batch_xs_label=test_labels[i*cls_batch_size:(i+1)*cls_batch_size]\n",
    "        predicts= sess.run(logits,\n",
    "                                 feed_dict={x_hat: batch_xs_input,x_label:batch_xs_label,keep_prob : 1})\n",
    "        num+=np.sum(one_hot[np.argmax(batch_xs_label,axis=1)]*one_hot[np.argmax(predicts,axis=1)],axis=0)\n",
    "        corrects+=np.sum(np.argmax(predicts,axis=1)==np.argmax(batch_xs_label,axis=1))\n",
    "        #         loss_np+=cls_loss_np\n",
    "    print('acc %3.2f '% (corrects/batchs/cls_batch_size))\n",
    "    print('recall : ',num/den)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
